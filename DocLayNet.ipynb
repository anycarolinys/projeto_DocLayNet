{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importação e Visualização dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade huggingface_hub\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "FIG_NUMBER = 1\n",
    "IMG_HEIGHT = 512\n",
    "IMG_WIDTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = load_dataset(\"pierreguillou/DocLayNet-base\", trust_remote_code=True)\n",
    "# dataset = load_dataset(\"pierreguillou/DocLayNet-small\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = dataset['train'].to_pandas()\n",
    "df_validation = dataset['validation'].to_pandas()\n",
    "df_test = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Distribuição dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_categories = df_train.doc_category.unique()\n",
    "num_classes = len(doc_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "def plot_image_distribution(dataframe, axis, base_name, color):\n",
    "    global FIG_NUMBER\n",
    "    num_images = []\n",
    "    for i in range(num_classes):\n",
    "        class_data = dataframe[dataframe['doc_category'] == doc_categories[i]]\n",
    "        count = class_data.shape[0]\n",
    "        num_images.append(count)\n",
    "\n",
    "    axs[axis].bar(doc_categories, num_images, color=f'{color}')\n",
    "    axs[axis].set_xticks(range(len(doc_categories)))\n",
    "    axs[axis].set_xticklabels(doc_categories, rotation=45)\n",
    "    axs[axis].set_title(f'Figura {FIG_NUMBER} - Base de {base_name}')\n",
    "    axs[axis].set_xlabel('Categorias')\n",
    "    axs[axis].set_ylabel('Imagens')\n",
    "    FIG_NUMBER += 1\n",
    "\n",
    "plot_image_distribution(df_train, 0, 'Treino', 'lightgreen')\n",
    "plot_image_distribution(df_validation, 1, 'Validação', 'mediumseagreen')\n",
    "plot_image_distribution(df_test, 2, 'Teste', 'darkgreen')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_data = df_train[df_train['doc_category'] == 'manuals']\n",
    "print(class_data.shape[0])\n",
    "\n",
    "class_data = df_validation[df_validation['doc_category'] == 'manuals']\n",
    "print(class_data.shape[0])\n",
    "\n",
    "class_data = df_test[df_test['doc_category'] == 'manuals']\n",
    "print(class_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - A distribuição das categorias de imagens é relativamente semelhante entre as bases, a categoria com mais disparidade é ***manuals***, sendo:  \n",
    "> **1422** para a base de **treino**  \n",
    "> **180** para a base de **validação**  \n",
    "> **77** para a base de **teste**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pré Processamento da Imagem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['image'][0]['bytes'])\n",
    "# array = np.array(df_train['image'][0]['bytes'])\n",
    "# array = np.frombuffer(df_train['image'][0]['bytes'])\n",
    "# array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_png_array(df_row):\n",
    "    png_bytes = df_row['bytes']\n",
    "\n",
    "    # Converta os bytes da imagem em um objeto Image\n",
    "    image = Image.open(io.BytesIO(png_bytes))\n",
    "    \n",
    "    grayscale_image = image.convert('L')\n",
    "    \n",
    "    # Redimensione a imagem\n",
    "    resized_image = grayscale_image.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "    # Converta a imagem redimensionada para um array NumPy\n",
    "    image_array = np.array(resized_image)\n",
    "    \n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['image_array'] = df_train['image'].apply(lambda x: get_png_array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation['image_array'] = df_validation['image'].apply(lambda x: get_png_array(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementação da Rede Neural Artificial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = df_train['image_array']\n",
    "# y_train = df_train['doc_category']\n",
    "x_train = np.stack(df_train['image_array'].values).astype(np.float32) / 255.0\n",
    "x_val = np.stack(df_validation['image_array'].values).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((x_train.shape[0], IMG_WIDTH * IMG_HEIGHT))\n",
    "x_val = x_val.reshape((x_val.shape[0], IMG_WIDTH * IMG_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['doc_category']\n",
    "y_val = df_validation['doc_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "\n",
    "y_train_encoded = label_binarizer.fit_transform(y_train)\n",
    "y_val_encoded = label_binarizer.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Rede Neural Artificial Densa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade keras\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas para criação e treinamento dos modelos\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando tf.data.Dataset\n",
    "batch_size = 500\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train_encoded))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val_encoded))\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_modelv2 = Sequential([\n",
    "    Flatten(input_shape=(IMG_HEIGHT*IMG_WIDTH,)),\n",
    "    Dropout(0.2),\n",
    "    Dense(200, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "adam = Adam(learning_rate=0.001)\n",
    "neural_network_modelv2.compile(optimizer=adam,\n",
    "              loss=[\"categorical_crossentropy\"],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "neural_network_modelv2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor='loss', patience=5)\n",
    "neural_network_historyv2 = neural_network_modelv2.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    # verbose=0,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" callback = EarlyStopping(monitor='loss', patience=5)\n",
    "neural_network_historyv2 = neural_network_modelv2.fit(np.array(x_train),\n",
    "          np.array(y_train_encoded),\n",
    "          epochs=100,\n",
    "          batch_size=500,\n",
    "          verbose=0,\n",
    "          shuffle=True,\n",
    "          validation_data=(np.array(x_val), np.array(y_val_encoded)),\n",
    "          callbacks=[callback]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" fig, axs = plt.subplots(1,2,figsize=(10,8))\n",
    "axs[1, 0].plot(neural_network_historyv2.history['accuracy'], color='violet', label='accuracy')\n",
    "axs[1, 0].plot(neural_network_historyv2.history['val_accuracy'], color='darkviolet', label='val_accuracy')\n",
    "axs[1, 0].set_title('Acurácia Rede Neural v2', fontsize=10)\n",
    "\n",
    "axs[1, 1].plot(neural_network_historyv2.history['loss'], color='orange',label='loss')\n",
    "axs[1, 1].plot(neural_network_historyv2.history['val_loss'], color='tomato', label='val_loss')\n",
    "axs[1, 1].set_title('Perda Rede Neural v2', fontsize=10)\n",
    "\n",
    "axs[1, 0].legend(loc=\"upper left\")\n",
    "axs[1, 1].legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show() \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
